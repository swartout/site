---
title: CSE 478
---

# CSE 478

---

...

## Lecture 4, April 1

- Motion models:
    - Capture change in state, $P(x | a)$
    - We can use simple models, adding some noise to account for errors
    - Good models incorporate it's uncertainty
    - Better to overstate uncertainty
- Our MuSHR motion model:
    - Kinematic: assumes "speed", not acceleration, is the action
    - Assume perfect, flat ground
    - State: vec of x, y, theta (pos, heading)
    - Controls: speed, steering angle
    - New state is old state plus the delta
    - Noise:
        - Add noise to control before propagating through the model
        - We add noise to state after the new prediction
- Euler's Theorem: TLDR; for any moving, turning object, there is a "center of rotation", where all of the forces on the car are perpendicular to that center
- Equations:
    - $\dot{\theta} = \w = \frac{v}{L} \tan \delta$
        - Change in direction
- MuSHR sensor model:
    - $P(z_t | x_t) \rightarrow P(z_t | x_t, m)$
        - $z_t$: observation (laser scan), $x_t$: state, $m$: map
    - Sources of stochasticity:
        1. Measurement noise (gaussian around measurement)
        2. Unexpected objects, e.g. map is wrong and someone is in between (add probability between 0 and sensor reading)
            - In class, we'll use linear probability between zero and obs
        3. Sensor failures (add some noise at $z_\text{max}$)
        4. Random measurements (add uniform random measurement)

---

## Lecture 5, April 3

- Monte-Carlo methods: random sampling to estimate expectation
- Importance sampling:
    1. Sample from a simple distribution
        - Take samples from proposal dist $q(x)$
    2. Reweight the samples to match target distribution $p(x)$
        - Reweight samples by $\frac{p(x)}{q(x)}$
- Belief updating:
    - Wikihow How-To:
        1. Predict our current beliefs forward with motion model
        2. Reweight our beliefs with observation model
        3. Renormalize our beliefs
    - Resample from our beliefs to prevent getting stuck
- Particle samping issues:
    - Two-room issue:
        - Beliefs will coalesce in a feedback loop
        - E.g. fix: resample less if equal confidence in most weights
        - Fix: low-variance resampling, take initial point and then uniform steps
- Can have variable amounts of particles

---

## Lecture 6, April 8

- Kalman Filtering: belief is a Gaussian, linear Gaussian systems
    - Works with continuous distributions
    - Paramaturized by $\mu_t$ and $\sigma_t^2$
    - Works with linear models with Gaussian noise
    - $x_t = ax_{t-1} + bu_t + \mathcal{N}(0, \sigma_r^2)$
    - $z_t = cx_t + \mathcal{N}(0, \sigma_q^2)$
- Kalman gain: $\frac{\text{Var(estimate)}{\text{Var(estimate) + Var(measurement)}$
- In multiple dimensions, we start to use covariance instead of variance
- $x_t$ is a distribution, but often used in equations as the mean of the distribution
- More efficient than particle filters, but requires linear motion and sensor models

---

## Lecture 7, April 10

- We are not studying the extended Kalman filter
    - Requires us to compute the Jacobian
    - Has issues with highly non-linear functions, only the mean gets the nonlinear update
- Unscented Kalman filter:
    - Easy to get the mean of the new Gaussian - just run the old mean through the non-linear update
    - Getting the variance is harder, we don't have an easy exact way to do so
    - Instead, we model the distribution (and get it's variance) via sampling:
        - We get $2n+1$ "sigma points", where $n$ is the dimensionality of the state vector
        - First sigma point is the mean
        - $x_i = \mu + (\sqrt{(n+\lambda)\Sigma})_i$ for $i=1,\dots,n$
        - $x_i = \mu - (\sqrt{(n+\lambda)\Sigma})_{i-n}$ for $i=n+1,\dots,2n$
        - Weight the mean higher, because we know it is true
