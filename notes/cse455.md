---
title: CSE 455 (Computer Vision)
---

# CSE 455, Computer Vision

## Spring 2024, Ranjay Krishna

---

## Lecture 1, March 26

- Two big technologies make computer vision possible:
    - Cameras: easy way to capture images
    - Ability to scan and digitize images
- Goal of CV: convert light into meaning!
    - Very multidiciplinary definition
- Tasks
    - Extracting *semantic* information
    - Extracting *geometric* information
- 1966: one undergrad at MIT solved CV in a summer!
- 80% of web traffic is images and videos - very interesting fact!
- First big sucess of CV was face detection for cameras
- We're going to focus on non-deep learning CV
    - CV is not *just* DL! We have to find what's important in the future!

---

## Lecture 2, March 28

- Color is essential to understand the world - computers need thatt capability too!
- Simple color can be used for clustering or object recognition
- Color formation:
    - Light can be complete described by a plot/histogram of wavelengths vs quantity
    - Light is subtractive or additive
- Radience: amount of light reflected onto an object
- Irradiance: amount of energy onto an object
- BRDF: function between radience and irradiance
- Diffuse surfaces: appear equally bright from all directions (light is spread out equally)
- Albedo: fraction of light a surface reflects
- How do humans see light?
    - Cones: less sensitive to brightness, color
        - Three different kinds of cones, each tuned to different wavelengths
    - Rods: more sensitive to brightness, grayscale
- Color is very lossy and we lose a lot of information when light hits things!
- Color spaces:
    - Basis are "basis colors" (e.g. RGB)
    - Mixing two colors allows for a linear combination of colors between them
    - Mixing three colors can't create all possible colors!
    - XYZ colorspace:
        - Y is the *brightness* of the color
    - HSV colorspace:
        - Hue
        - Saturation
        - Value (intensity)

---

## Lecture 3, April 2

- Resolution (quantization) is a major loss of information
- Metric is DPI (dots per inch)
- Types of images:
    - Binary (white and black)
    - Greyscale
    - Color: three channels
- Histograms are just counts over an image
    - This can be done over an entire image, over patches, over rows/columns, etc
- Images as functions:
    - Converts the real world into pixel values
    - Digital images are typically discrete
    - These functions can convert positions into values, one for each channel
    - We assume positions outside the "image" are NaN
    - $\mathcal{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^C$, where $C$ is number channels
    - Domain is: $-\inf, \inf$, range is: $0, 255$
- Filters transform images:
    - $\text{image} \rightarrow \text{image}$ function
    - Do some sort of transformation, e.g. denoising
    - All layers in a neural network are simply filters
    - There are linear filters! (Very similar to NNs)
- A system might use multiple filters to transform an image
- Possible properties of systems:
    - Additivity
    - Homogenity
    - Superposition: basically, does it work as a linear combination?
    - Stability: if input is bounded, the output is also bounded
    - Invertibile
    - Causality: all values upper and to the left are zero, the output is zero
    - Shift invariance: shifted input and output are shifted by the same amount

---

## Lecture 4, April 4

- A filter is a linear system if superposition holds
- Linear shift invariant systems (LSI) also are shift invariant
- Humans are (practically) scale and transition invariant (cool!)
- Inpulse function: one at the origin, zero everywhere else
    - $\delta_2$ is a 2-D impulse function
    - $h[m, n]$ is the output at $m, n$ of the inpulse function after a filter
    - Any filter can be written as a summation of shifted delta functions!
- Property of LSI systems:
    - For an input inpluse function into a LSI system, the LSI system is complelty specified by the outptut
    - We can use that output instead of the LSI system (allows convolutions!)
    - This means for: $f[n, m] \overset{S}{\rightarrow} g[n, m]$, $f[n, m] = \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf C_{k, l} \cdot \delta_2 [n-k, m-l]$
    - Finally, $\overset{S}{\rightarrow} \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf C_{k, l} \cdot S\{\delta_2 [n-k, m-l]\}$
    - This is the convolution operation!
- $f[n,m]*h[n,m] = \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf f[k,l] \cdot h[n-k, m-l]$
    - $*$ typically represents the convolution operation
- Convolutions:
    - We will typically "flip" or "fold" the kernel about the origin, so that it works with our equation
- We can have complicated transformations (e.g. sharpening) just via multiple kernels!
- For finite images on computers with non-infinite domain, we typically pad the edges
    - Can pad with zeros, mean, mirror, extension, etc
- Cross-correlation:
    - Two-star symbol: $**$
    - Convolution without the flip
    - Similar to the search operation
    - What is typically implemented in DL(?)

---

## Lecture 5, April 9

- Systems can be represented as a sum of inpluse responses
- LSI systems are completely specified by their inpluse responses
- We can create new systems by combining filters
- Edges are essential in the way we visually understand 
    - Some edges are more important for specific image types
- Very cool example: classified images based on brain scans, while people were looking at images. Little difference between showing a human a RGB photo vs black and white edges.
- Edges also allow much more efficient processing
- Edge detection:
    - Identify sudden changes in an image
    - We often want to find physical edges, not necessarily color discontinuities
    - We can calculate numeric derivatives in images and use them to find edges
    - Filters can perform differentiation
- Derivatives: backward, forward, and central derivatives
- Discrete derivation in 2D:
    - Output two gradients from both dimensions and combine them
    - Edge strenth comes from: $\sqrt{f_n^2 + f_m^2}$
    - We can get the direction via: $\theta = \tan^{-1}(\frac{f_m}{f_n})$
    - Gradient spikes will be edges! (assuming the image isn't noisy)
    - To combat noise, we can add a blur kernel to the image
- Edge detection example:
    - Add gasussian smoothing kernel
    - Add derivative kernel afterwards
    - This can be done in one step! (derivative of the gaussian kernel, DoG)

---

## Lecture 6, April 11

- Sobel Filters:
    - Gaussian blur combined with central derivative filter
    - Has poor localization
    - Requires you to threshold the values - a messy problem
- Canny edge detector:
    - From 1972!
    - Most common edge detector today
    - Theoretically optimal when pixels have Gaussian noise
    - Steps:
        1. Supress noise
        2. Compute gradient magnitude and direction
        3. Apply non-maximum supression
        4. Hysteresis
    - Non-maximum supression
        - Zero out non-maximum edge values
        - For any point, compare it to neighbors. If it is smaller, zero it out
        - Get the two neighbors via the gradient angle
        - As pixels are quantized, can use weighted average of neighboring eight pixels
    - Hysteresis:
        - Two thresholds, low and high
        - Low thresholds become and edge if it connects two strong edges
- Hough transform ("Huff"):
    - 1962
    - Transform edges into lines
    - For a single point, we can define lines that pass through it in $a,b$ (slope-bias) space
    - Do this for all points, intersection (or close intersections) represent lines going through the same points
    - Divide the $a,b$ space into cells, count the number of intersections, draw those lines
    - Can use one of many basis, e.g. polar
    - Could be extended to multiple dimensions, for curves in images
    - Computationally complex
    - $O(n^2)$ of edges
- RANSAC:
    - Try to remove the votes of noisy edges, without $O(n^2)$ iteration
    - Bad when many outliers
    - Steps:
        1. Pick some points at random, draw a line between them
        2. Find the "inlier" points are within a threshold from the line
        3. If the number of inliers is greater than the current greatest number, compute line of best fit *from the inliers*
        4. Repeat until number of inliers doesn't change
        5. Repeat all $k$ times
    - Hyperparameters:
        - Number of points in the seed set
        - How many times to repeat? ($k$)
        - Threshold
        - Min number of inliers to claim a line exists

---

## Lecture 7, April 16

- Simple cross-correlation doesn't work for some matching problems:
    - Occlusion, lighting shifts, intra-category variation, articulation, etc.
- Matching images, a formula:
    - Take patches and match them to an image
    - General idea: find key points and the regions surrounding them. Describe and normalize them, then compute descriptors.
- Harris Corner Detector:
    - Idea is multiplying gradients of $x$ and $y$ by each other
    - "Find patches resulting in large pixel-value changes in *any* direction"
    - We create energy function: $E(u,v) = \sum_{x,y} w(x,y) [I(x+u,y+v)-I(x,y)]^2$
        - Window function might be Gaussian or discrete
        - We can use the Taylor expansion, see slides for formula
        - This represents an ellipse
        - We use eigenvalues to find rotations, but approximate the eigenvalues
    - Algo:
        1. Compute image derivatives
        2. Square of derivatives
        3. Gaussian filter
        4. Cornerness function (ensure two large eigenvalues)
        5. Non-maxiumum supression (only one corner per location)
        6. Threshold
    - Properties:
        - Translation invariant
        - Rotation invariant
        - *Not* scale invariant
