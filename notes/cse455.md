---
title: CSE 455 (Computer Vision)
---

# CSE 455, Computer Vision

## Spring 2024, Ranjay Krishna

---

## Lecture 1, March 26

- Two big technologies make computer vision possible:
    - Cameras: easy way to capture images
    - Ability to scan and digitize images
- Goal of CV: convert light into meaning!
    - Very multidiciplinary definition
- Tasks
    - Extracting *semantic* information
    - Extracting *geometric* information
- 1966: one undergrad at MIT solved CV in a summer!
- 80% of web traffic is images and videos - very interesting fact!
- First big sucess of CV was face detection for cameras
- We're going to focus on non-deep learning CV
    - CV is not *just* DL! We have to find what's important in the future!

---

## Lecture 2, March 28

- Color is essential to understand the world - computers need thatt capability too!
- Simple color can be used for clustering or object recognition
- Color formation:
    - Light can be complete described by a plot/histogram of wavelengths vs quantity
    - Light is subtractive or additive
- Radience: amount of light reflected onto an object
- Irradiance: amount of energy onto an object
- BRDF: function between radience and irradiance
- Diffuse surfaces: appear equally bright from all directions (light is spread out equally)
- Albedo: fraction of light a surface reflects
- How do humans see light?
    - Cones: less sensitive to brightness, color
        - Three different kinds of cones, each tuned to different wavelengths
    - Rods: more sensitive to brightness, grayscale
- Color is very lossy and we lose a lot of information when light hits things!
- Color spaces:
    - Basis are "basis colors" (e.g. RGB)
    - Mixing two colors allows for a linear combination of colors between them
    - Mixing three colors can't create all possible colors!
    - XYZ colorspace:
        - Y is the *brightness* of the color
    - HSV colorspace:
        - Hue
        - Saturation
        - Value (intensity)

---

## Lecture 3, April 2

- Resolution (quantization) is a major loss of information
- Metric is DPI (dots per inch)
- Types of images:
    - Binary (white and black)
    - Greyscale
    - Color: three channels
- Histograms are just counts over an image
    - This can be done over an entire image, over patches, over rows/columns, etc
- Images as functions:
    - Converts the real world into pixel values
    - Digital images are typically discrete
    - These functions can convert positions into values, one for each channel
    - We assume positions outside the "image" are NaN
    - $\mathcal{f}: \mathbb{R}^2 \rightarrow \mathbb{R}^C$, where $C$ is number channels
    - Domain is: $-\inf, \inf$, range is: $0, 255$
- Filters transform images:
    - $\text{image} \rightarrow \text{image}$ function
    - Do some sort of transformation, e.g. denoising
    - All layers in a neural network are simply filters
    - There are linear filters! (Very similar to NNs)
- A system might use multiple filters to transform an image
- Possible properties of systems:
    - Additivity
    - Homogenity
    - Superposition: basically, does it work as a linear combination?
    - Stability: if input is bounded, the output is also bounded
    - Invertibile
    - Causality: all values upper and to the left are zero, the output is zero
    - Shift invariance: shifted input and output are shifted by the same amount

---

## Lecture 4, April 4

- A filter is a linear system if superposition holds
- Linear shift invariant systems (LSI) also are shift invariant
- Humans are (practically) scale and transition invariant (cool!)
- Inpulse function: one at the origin, zero everywhere else
    - $\delta_2$ is a 2-D impulse function
    - $h[m, n]$ is the output at $m, n$ of the inpulse function after a filter
    - Any filter can be written as a summation of shifted delta functions!
- Property of LSI systems:
    - For an input inpluse function into a LSI system, the LSI system is complelty specified by the outptut
    - We can use that output instead of the LSI system (allows convolutions!)
    - This means for: $f[n, m] \overset{S}{\rightarrow} g[n, m]$, $f[n, m] = \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf C_{k, l} \cdot \delta_2 [n-k, m-l]$
    - Finally, $\overset{S}{\rightarrow} \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf C_{k, l} \cdot S\{\delta_2 [n-k, m-l]\}$
    - This is the convolution operation!
- $f[n,m]*h[n,m] = \sum_{k=-\inf}^\inf \sum_{l=-\inf}^\inf f[k,l] \cdot h[n-k, m-l]$
    - $*$ typically represents the convolution operation
- Convolutions:
    - We will typically "flip" or "fold" the kernel about the origin, so that it works with our equation
- We can have complicated transformations (e.g. sharpening) just via multiple kernels!
- For finite images on computers with non-infinite domain, we typically pad the edges
    - Can pad with zeros, mean, mirror, extension, etc
- Cross-correlation:
    - Two-star symbol: $**$
    - Convolution without the flip
    - Similar to the search operation
    - What is typically implemented in DL(?)

---

## Lecture 5, April 9

- Systems can be represented as a sum of inpluse responses
- LSI systems are completely specified by their inpluse responses
- We can create new systems by combining filters
- Edges are essential in the way we visually understand 
    - Some edges are more important for specific image types
- Very cool example: classified images based on brain scans, while people were looking at images. Little difference between showing a human a RGB photo vs black and white edges.
- Edges also allow much more efficient processing
- Edge detection:
    - Identify sudden changes in an image
    - We often want to find physical edges, not necessarily color discontinuities
    - We can calculate numeric derivatives in images and use them to find edges
    - Filters can perform differentiation
- Derivatives: backward, forward, and central derivatives
- Discrete derivation in 2D:
    - Output two gradients from both dimensions and combine them
    - Edge strenth comes from: $\sqrt{f_n^2 + f_m^2}$
    - We can get the direction via: $\theta = \tan^{-1}(\frac{f_m}{f_n})$
    - Gradient spikes will be edges! (assuming the image isn't noisy)
    - To combat noise, we can add a blur kernel to the image
- Edge detection example:
    - Add gasussian smoothing kernel
    - Add derivative kernel afterwards
    - This can be done in one step! (derivative of the gaussian kernel, DoG)
