---
title: Distributed DL Training
---

# Distributed DL Traning

[repo](https://github.com/swartout/distributed)

## Log:

### Oct 4

Starting to write a distributed DL training repo from scratch. I'll be using torchtitan as a reference when necessary, but trying to do as much myself as possible. The goal of this is to learn and get some experience how to do "serious" pytorch training.

I'll be using Llama 3 as the implementation and run the code on UW's Hyak supercomputer.

My first goal is to implement Llama 3 and verify it loads properly.
