---
title: CSE 447 (NLP)
---

# CSE 447 (NLP)

## Yejin Choi, Winter 2024

---

## Lecture 1 - Jan 3

**Logistics:**

- No final exam
- Four assignments (plus one optional)
- Final project
    - Either open-ended research project or replicating/extending papers
- Small participation scores

**Content:**

- Course materials have been revamped because of LLMs
- Went over common use cases of LLMs, failure modes

---

## Lecture 2 - Jan 5

**Classical NLP day!**

- Language modeling task: estimate a probability function for all possible
  strings
- Idea: use an emperical distribution over traning sentences
    - No generalization (literally cannot be out-of-distribution)
- BoW (Unigram) models:
    - Append a `STOP` token to all sentences
    - Multiply probability of all tokens
    - **Not** conditional
    - Word order doesn't matter
- Bigram models:
    - Append `STOP` token and prepend `START` token
    - Conditioned on previous token
    - Same sort of autoregressive generation
- N-gram models:
    - Condition on the last n tokens when generating next token
    - Practically cannot go beyond 3 or 4-gram, Google got ~9-gram
    - Learned size is: $|\text{dict size}|^N$
        - Grows exponentially
    - This is the MLE

---

## Lecture 3 - Jan 8

**Finish classical NLP day!**

- While the data we have for N-gram models doesn't change, the data per
  situation will decrease as N increases (and thus cause problems)
- *"The Shannon Game"*
    - How well can we predict the next word?
    - Given a prefix, what's the next word?
- We use typical ML/DL data splits for performance evaluation
    - Train, dev, test sets
- Measures of fit:
    - Likelihood, (negative) log-liklihood
    - Perplexity:
        - Inverse probability of the data, normalized by number of tokens
        - $PP_M(D') = p_M(D')^{-\frac{1}{N}}$
        - "Geometric mean"
        - The amount of "surprise" in the data
        - Exponentiated cross-entropy! $PP(X) = \exp(H(X))$
        - Example: perplexity of Uniform with $i$ outputs is $i$ (kinda like how
          hard is it to recognize)
        - Perplexity represents the branching factor of data
        - Frontier LLMs have perplexity $<10$
    - Cross-entropy:
        - For two prob distributions $P, Q$ you want to compare, cross entropy
          is: $H(P, Q) = -\mathbb{E}_p[\log q] = - \sum_{x \in X} p(x) \log
          q(x)$
        - Non-symmetric
        - If we want to model natural language distribution $L$ and fit $P_M$ to
          minimize cross-entropy: $H(L, P_M)$
            - Use *Monte-Carlo* estimate for $L$, assume $l(x_i \simeq
              \frac{1}{N}$ over some dataset $D$ drawn from $L$
            - Basically just saying all phrases have an equal chance of
              appearing
            - $H(L, P_M) \simeq = -\sum_{i=1}^N \frac{1}{N} \log p_M(x_i | x_0,
              \dots, x_{i-1})$
    - Intrinsic evaluation: more about measuring facts about the underlying
      model
    - Extrinsic evaluation: more application and context focused
- Token distribution follows Zipf's distribution, kinda power-law

---

## Lecture 4 - Jan 10

- We can never have a probability of zero for models:
    - Laplace smoothing: (add one)
    - Add-k: add some $0 < k < 1$
        - Better but still not used
    - Convex combinations (?)
        - Take combination of multiple distributions
        - Better than add-k typically
        - Optimal hyperparameter search: human guessing
        - E.g. linear combination of 1-gram, 2-gram, 3-gram distributions
- Unknown token `<UNK>`, known as "UNC!"
    - `<UNK>`
    - Token representing out-of-vocab tokens
    - Model can generate `<UNK>` as well

**Finally done with lecture 2!!!**

- 470,000 words in Webster's
- Character level tokenization:
    - Each token is a character
- BPE:
    - Tokenize subwards, similar to a compression algorithm
    - We kinda did this in CSE 143
    - Greedly concatenating most common token-pairs starting from chars
    - Common way to tokenize natural text, perform the same merges as when
      designing the BPE encoding

---

## Lecture 5 - Jan 12

- Tokenizers really fit data with weird matchings
- Whitespace and capitalization will typically cause differnt tokenizations
- Tokenization varients:
    - Strip whitespace, add spaces during decoding. Requires a "continue word"
      special character.
    - No pretokenization:
        - Sentencepiece tokenization
            - Generally used for non-english, char-based languages
    - Byte-based variations!
        - Using emojis!
    - WordPiece: add bigram tokens by normalized probability of occuring
    - Tokenizer-free modelling
- One-hot encoding: vector of all zeros, with a one representing index
- Embeddings: basically a differentiable lookup table for token representations
    - Have relationships in latent space
- Basic NN review:
    - Just simple basics

---

## Lecture 6 - Jan 17

*Guest Lecture - Peter West*

- "Large" in LLMs: ~GPT-2 size or greater
- Commonsense knowledge:
    - Is commonsense knowledge too hard for compact models?
    - Can we distill commonsense knowledge to smaller models?
- Knowledge distillation:
    - Minimize cross-entropy between teacher and student models
        - Can't do all strings, instead do over subset of important data
    - We get a knowledge graph (DS) and student model as output
- Used GPT-3 to produce massive corpus of commonsense examples:
    - Human baseline producing examples: ~86%
    - Produced much more (10X) data, with 10% drop in example accuracy
    - Attempted to use RoBERTa to filter data
        - Increased acc to 10% more than humans, while dropping examples to
          about half
- Got final student model by finetuning GPT-2 on synthetic data
    - Can improve over baseline GPT-3 model
- Humans are good supervisors, not creators (P-NP)
- Spoke about how models can generate better than reason about outputs
    - Kinda a reverse P-NP vs humans
