---
title: CSE 447 (NLP)
---

# CSE 447 (NLP)

## Yejin Choi, Winter 2024

---

## Lecture 1 - Jan 3

**Logistics:**

- No final exam
- Four assignments (plus one optional)
- Final project
    - Either open-ended research project or replicating/extending papers
- Small participation scores

**Content:**

- Course materials have been revamped because of LLMs
- Went over common use cases of LLMs, failure modes

---

## Lecture 2 - Jan 5

**Classical NLP day!**

- Language modeling task: estimate a probability function for all possible
  strings
- Idea: use an emperical distribution over traning sentences
    - No generalization (literally cannot be out-of-distribution)
- BoW (Unigram) models:
    - Append a `STOP` token to all sentences
    - Multiply probability of all tokens
    - **Not** conditional
    - Word order doesn't matter
- Bigram models:
    - Append `STOP` token and prepend `START` token
    - Conditioned on previous token
    - Same sort of autoregressive generation
- N-gram models:
    - Condition on the last n tokens when generating next token
    - Practically cannot go beyond 3 or 4-gram, Google got ~9-gram
    - Learned size is: $|\text{dict size}|^N$
        - Grows exponentially
    - This is the MLE
