---
title: CSE 422 (Modern Algos)
---

# CSE 422 (Modern Algos)

## James R. Lee, Winter 2024

---

## Lecture 1 - Jan 4

- 9 topics + a bonus topic, one per week
- One mini-project per week, always due on Wed
    - Can work with partners
- "Power of two choices"
    - Related problem:
        - Put $n$ balls in $n$ bins, i.i.d.
        - "What is the expected number of balls in the most-full bin?"
        - It grows via: $\sim \frac{\log n}{\log \log n}$
    - Two-choice strategy:
        - Chose two bins $i, j \in {1, \dots, n}$ uniformly at random
        - Throw ball in the least loaded of the two!
        - Grows via: $\sim \log \log n$
    - $d$-choices: $\sim \frac{\log \log n}{\log d}$
        - The same "boost" doesn't happen again outside of $1 \rightarrow 2$

---

## Lecture 2 - Jan 9

- Basic web caching:
    - Akamai was one of the first companies
    - Basic web use: use a url to request data from a server
    - Caching: use a intermediary cache to request data first
- Hash functions:
- Consistent hashing problem: given a cache distributed over $N$ machines, how
  to quickly figure out which machine a string $x$ is associated with, while
  balancing load between machines
    - Motivated by web-caching: how can we cache data at URLs?
    - The basic solution would be to use a typical hash function $H(\cdot)$ and
      modulo: $H(x) \mod N$
    - However we need to agree to a fixed $N$, and in real, dynamic
      environments we cannot do so
        - Changing $N$ would have a cost of $1 - \frac{1}{N}$
- "Circular hashing solution":
    - (Assume the hash outputs are 32-bit for this problem)
    - Hash each server, place them on a $2^32$ circle
    - When caching data $x_0$, find hash $H(x_0)$, then go clockwise to first
      hashed server. This is the cache server
    - Expected load per server: $\frac{P}{N}$ where $P$ is the number of objects
    - When we add another server, we only need to relocate $\frac{1}{N}$ objects
- How to do the "clockwise scan"?
    - We want some datastructure which can find the server $v'$ for a has $v$,
      such that $v' \geq v$
    - Balanced binary search trees (e.g. red-black trees) can do this in $O(\log
      N)$ time
    - If $N$ is small, we can also use other data structures
- Variance reduction trick:
    - Using basic method, some servers can get overloaded
    - Trick: compute $k$ hashes for each server (filling out the circle)
    - If $k \simeq \log N$, there are theoretical bounds that servers won't be
      overloaded with high probability
    - This also helps when we have servers with differing memory capacities,
      server with 2x memory can get 2x the number of hash-keys!

---

## Lecture 3 - Jan 9

- Majority element problem: given an array of length $n$ that has a majority
  element, (more than $\frac{n}{2}$), find that element!
    - Simple algo to find linear-time algorithm, keep a counter of most-seen
      element, increment/decrement it depending on what is seen
- Heavy hitters problem: given an array $A$ of *large* length $n$ and a smaller
  $k$, compute the values which appear at least $\frac{n}{k}$ times.
    - Many applications: given a large stream of data, compute elements which
      appear a certain amount of times
    - If $A$ can fit in memory: trivial $O(n \log n)$, sort array, find which
      elements appear at least $k$ times in a row
    - **There is no algo which solves Heavy Hitters in one pass with sublinear
      amount of auxillary space**
        - We cannot store all counts, therefore if possible candidates change,
          we won't know the counts of items which could replace it
- $\epsilon$-approximate heavy hitters problem:
    - Input: same as HH, but with additional parameter $0 < \epsilon < 1$
    - Output: a list where:
        - All values in $A$ which occur at least $\frac{n}{k}$ times are in the
          list
        - All values in the list occur at least $\frac{n}{k} - \epsilon n$ times
          in $A$ (provides margin of error)
    - Auxillary memory required grows as $\frac{1}{\epsilon}$
        - We can't have $\epsilon = 0$ as the memory requirement grows to
          infinity
- Count-min sketch: a datastructure for $\epsilon$-approximate heavy hitters
    - Supports two operations:
        - `inc(x)`: increment the count of `x`
            - `for all i = 1, 2, ..., L: arr[i][hash_i(x)] += 1`
            - $O(\ell)$ time
        - `count(x)`: get the count of `x`
            - `return min_i arr[i][hash_i(x)]`
            - $O(\ell)$ time
    - Parameters:
        - Number of buckets $b$ (larger than $\ell$, much smaller than $n$)
        - Number of hash functions $\ell$ (smaller)
    - Data structure: $\ell \times b$ 2-D array of ints, init at 0
    - The array counts can only *underestimate* the true counts - the error is
      one-sided
    - Given reasonable choices of parameters, the expected error rate is
      $\frac{1}{2}^\ell$
- Role model: bloom filters
    - Remembers which elements have been inserted
    - Has false positives! Sometimes confirms elements which weren't added
    - No false negatives however
    - 1-byte per element results in false positive rate of less than 2%
    - Represents accuracy-space tradeoff

